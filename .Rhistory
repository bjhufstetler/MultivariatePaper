load("~/MultivariatePaper/pmesi.RData")
View(pmesi.final)
View(pmesi.final)
table(adult$Highest Level of Conflict Intensity)
table(adult$"Highest Level of Conflict Intensity"")
>
table(adult$"Highest Level of Conflict Intensity")
table(pmesi.final$`Highest Level of Conflict Intensity`)
count(is.na(pmesi.final$`Highest Level of Conflict Intensity`))
sum(is.na(pmesi.final$`Highest Level of Conflict Intensity`))
table(is.na(pmesi.final$`Highest Level of Conflict Intensity`))
table(pmesi.final$`Highest Level of Conflict Intensity`=="")
is.na(pmesi.final$`Highest Level of Conflict Intensity`)
pmesi.final[981]
pmesi.final[981,]
pmesi.final[981,1:50]
# View the data
# Load in `ggvis`
install.packages("ggvis")
setwd("~/Documents/Brandon/AFIT/2019_Q3/IGMT_680 Database Management/Lectures/Week 03")
setwd("~/Documents/Brandon/AFIT/2019_Q3/IGMT_680 Database Management/Lectures/Week 03")
install.packages("caTools")
install.packages("kableExtra")
install.packages("rpart.plot")
#IRIS kNN EXAMPLE
#
setwd("~/Documents/Brandon/AFIT/2019_Q3/IGMT_680 Database Management/Lectures/Week 03")
df <- data(iris) ##load data
head(iris) ## see the studcture
# View the data
# Load in `ggvis`
# install.packages("ggvis")
library(ggvis)
# Iris scatter plot
# Sepal length vs width
iris %>% ggvis(~Sepal.Length, ~Sepal.Width, fill = ~Species) %>% layer_points()
# Petal length vs width
iris %>% ggvis(~Petal.Length, ~Petal.Width, fill = ~Species) %>% layer_points()
##Generate a random number that is 90% of the total number of rows in dataset.
ran <- sample(1:nrow(iris), 0.9 * nrow(iris))
##the normalization function is created
nor <-function(x) { (x -min(x))/(max(x)-min(x))   }
##Run nomalization on first 4 coulumns of dataset because they are the predictors
iris_norm <- as.data.frame(lapply(iris[,c(1,2,3,4)], nor))
summary(iris_norm)
##   Sepal.Length     Sepal.Width      Petal.Length     Petal.Width
##  Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min. :0.00
##  1st Qu.:0.2222   1st Qu.:0.3333   1st Qu.:0.1017   1st Qu.:0.08
##  Median :0.4167   Median :0.4167   Median :0.5678   Median :0.50
##  Mean   :0.4287   Mean   :0.4406   Mean   :0.4675   Mean   :0.45
##  3rd Qu.:0.5833   3rd Qu.:0.5417   3rd Qu.:0.6949   3rd Qu.:0.70
##  Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.00
##extract training set
iris_train <- iris_norm[ran,]
##extract testing set
iris_test <- iris_norm[-ran,]
##extract 5th column of train dataset because it will be used as 'cl' argument in knn function.
iris_target_category <- iris[ran,5]
##extract 5th column if test dataset to measure the accuracy
iris_test_category <- iris[-ran,5]
##load the package class
library(class)
##run knn function
pr <- knn(iris_train,iris_test,cl=iris_target_category,k=3)
##create confusion matrix
tab <- table(pr,iris_test_category)
tab
accuracy <- function(x){sum(diag(x)/(sum(rowSums(x)))) * 100}
accuracy(tab)
##because diamonds dataset is in ggplot2 package
library(ggplot2)
##load data
data(diamonds)
##store it as data frame
dia <- data.frame(diamonds)
head(dia)
##create a random number equal 90% of total number of rows
ran <- sample(1:nrow(dia),0.9 * nrow(dia))
##the normalization function is created
nor <-function(x) { (x -min(x))/(max(x)-min(x))   }
##normalization function is created
dia_nor <- as.data.frame(lapply(dia[,c(1,5,6,7,8,9,10)], nor))
##training dataset extracted
dia_train <- dia_nor[ran,]
##test dataset extracted
dia_test <- dia_nor[-ran,]
##the 2nd column of training dataset because that is what we need to predict about testing dataset
##also convert ordered factor to normal factor
dia_target <- as.factor(dia[ran,2])
##the actual values of 2nd couln of testing dataset to compaire it with values that will be predicted
##also convert ordered factor to normal factor
test_target <- as.factor(dia[-ran,2])
##run knn function
library(class)
pr <- knn(dia_train,dia_test,cl=dia_target,k=20)
##create the confucion matrix
tb <- table(pr,test_target)
##check the accuracy
accuracy <- function(x){sum(diag(x)/(sum(rowSums(x)))) * 100}
accuracy(tb)
boston = read.csv('boston.csv')
str(boston)
# Plot observations
plot(boston$LON, boston$LAT)
# Tracts alongside the Charles River
points(boston$LON[boston$CHAS==1], boston$LAT[boston$CHAS==1], col="blue", pch=19)
# Plot pollution/NOX
summary(boston$NOX)
points(boston$LON[boston$NOX>=0.55], boston$LAT[boston$NOX>=0.55], col="green", pch=20)
# Plot prices
plot(boston$LON, boston$LAT)
summary(boston$MEDV)
#Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
#5.00   17.02   21.20   22.53   25.00   50.00
points(boston$LON[boston$MEDV>=21.2], boston$LAT[boston$MEDV>=21.2], col="red", pch=20)
# Load CART packages
library(rpart)
# install rpart package
install.packages("rpart.plot")
# install rpart package
#install.packages("rpart.plot")
library(rpart.plot)
# CART model
latlontree = rpart(MEDV ~ LAT + LON, data=boston)
# Plot the tree using prp command defined in rpart.plot package
prp(latlontree)
# Visualize output
plot(boston$LON, boston$LAT)
points(boston$LON[boston$MEDV>=21.2],boston$LAT[boston$MEDV>=21.2], col="red", pch=20)
fittedvalues = predict(latlontree)
points(boston$LON[fittedvalues>21.2],boston$LAT[fittedvalues>=21.2], col="blue", pch="$")
#Simplifying Tree by increasing minbucket
latlontree = rpart(MEDV ~ LAT + LON, data=boston, minbucket=50)
plot(latlontree)
text(latlontree)
# Split the data
install.packages("caTools")
# Split the data
#install.packages("caTools")
library(caTools)
set.seed(123)
split = sample.split(boston$MEDV, SplitRatio = 0.7)
train = subset(boston, split==TRUE)
test = subset(boston, split==FALSE)
# Making a Regression Tree Model
# Create a CART model
tree = rpart(MEDV ~ LAT + LON + CRIM + ZN + INDUS + CHAS + NOX + RM + AGE + DIS + RAD + TAX + PTRATIO, data=train)
prp(tree)
tree.pred = predict(tree, newdata=test)
tree.sse = sum((tree.pred - test$MEDV)^2)
tree.sse
# Now use the IRIS dataset to classify iris type using CART
# load the package
library(rpart)
# load data
data(iris)
# fit model
fit <- rpart(Species~., data=iris)
# summarize the fit
summary(fit)
# make predictions
predictions <- predict(fit, iris[,1:4], type="class")
# summarize accuracy
iris_pred <- table(predictions, iris$Species)
iris_pred
##check the accuracy
accuracy <- function(x){sum(diag(x)/(sum(rowSums(x)))) * 100}
accuracy(iris_pred)
# Install packages/library and read in data
#install.packages("kableExtra")
library(kableExtra)
library(knitr)
credit.df <- read.csv('credit.csv')
credit.df <- read.csv('credit.csv')
credit.df$default <- factor(credit.df$default, levels = c(1, 2), labels = c('Yes', 'No'))
#View and understand you data
str(credit.df)
summary(credit.df)
kable(head(credit.df, n=15), format="html") %>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))
set.seed(1978)
credit.random.df <- credit.df[order(runif(1000)), ]
summary(credit.df$amount)
summary(credit.random.df$amount)
credit.train <- credit.random.df[1:900, ]
credit.test <- credit.random.df[901:1000, ]
prop.table(table(credit.train$default))
prop.table(table(credit.test$default))
library(C50)
credit.fit <- C5.0(credit.train[-17], credit.train$default)
credit.fit
summary(credit.fit)
# The output is the decision tree to determine whether or not a loan applicant is worthy of receiving a loan. It also shows which features are most important in determining which factors are most important when considering loan applications.
#IRIS kNN EXAMPLE
#
setwd("~/Documents/Brandon/AFIT/2019_Q3/IGMT_680 Database Management/Lectures/Week 03")
df <- data(iris) ##load data
head(iris) ## see the studcture
# View the data
# Load in `ggvis`
# install.packages("ggvis")
library(ggvis)
summary(iris)
# Iris scatter plot
# Sepal length vs width
iris %>% ggvis(~Sepal.Length, ~Sepal.Width, fill = ~Species) %>% layer_points()
# Petal length vs width
iris %>% ggvis(~Petal.Length, ~Petal.Width, fill = ~Species) %>% layer_points()
##Generate a random number that is 90% of the total number of rows in dataset.
ran <- sample(1:nrow(iris), 0.9 * nrow(iris))
##the normalization function is created
nor <-function(x) { (x -min(x))/(max(x)-min(x))   }
##Run nomalization on first 4 coulumns of dataset because they are the predictors
iris_norm <- as.data.frame(lapply(iris[,c(1,2,3,4)], nor))
summary(iris_norm)
##   Sepal.Length     Sepal.Width      Petal.Length     Petal.Width
##  Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min. :0.00
##  1st Qu.:0.2222   1st Qu.:0.3333   1st Qu.:0.1017   1st Qu.:0.08
##  Median :0.4167   Median :0.4167   Median :0.5678   Median :0.50
##  Mean   :0.4287   Mean   :0.4406   Mean   :0.4675   Mean   :0.45
##  3rd Qu.:0.5833   3rd Qu.:0.5417   3rd Qu.:0.6949   3rd Qu.:0.70
##  Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.00
##extract training set
iris_train <- iris_norm[ran,]
##extract testing set
iris_test <- iris_norm[-ran,]
##extract 5th column of train dataset because it will be used as 'cl' argument in knn function.
iris_target_category <- iris[ran,5]
##extract 5th column if test dataset to measure the accuracy
iris_test_category <- iris[-ran,5]
##load the package class
library(class)
##run knn function
pr <- knn(iris_train,iris_test,cl=iris_target_category,k=3)
##create confusion matrix
tab <- table(pr,iris_test_category)
tab
accuracy <- function(x){sum(diag(x)/(sum(rowSums(x)))) * 100}
accuracy(tab)
##Generate a random number that is 90% of the total number of rows in dataset.
ran <- sample(1:nrow(iris), 0.9 * nrow(iris))
##   Sepal.Length     Sepal.Width      Petal.Length     Petal.Width
##  Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min. :0.00
##  1st Qu.:0.2222   1st Qu.:0.3333   1st Qu.:0.1017   1st Qu.:0.08
##  Median :0.4167   Median :0.4167   Median :0.5678   Median :0.50
##  Mean   :0.4287   Mean   :0.4406   Mean   :0.4675   Mean   :0.45
##  3rd Qu.:0.5833   3rd Qu.:0.5417   3rd Qu.:0.6949   3rd Qu.:0.70
##  Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.00
##extract training set
iris_train <- iris_norm[ran,]
##extract testing set
iris_test <- iris_norm[-ran,]
##extract 5th column of train dataset because it will be used as 'cl' argument in knn function.
iris_target_category <- iris[ran,5]
##extract 5th column if test dataset to measure the accuracy
iris_test_category <- iris[-ran,5]
##run knn function
pr <- knn(iris_train,iris_test,cl=iris_target_category,k=3)
##create confusion matrix
tab <- table(pr,iris_test_category)
tab
accuracy <- function(x){sum(diag(x)/(sum(rowSums(x)))) * 100}
accuracy(tab)
##because diamonds dataset is in ggplot2 package
library(ggplot2)
##load data
data(diamonds)
##store it as data frame
dia <- data.frame(diamonds)
head(dia)
# Sepal length vs width
dia %>% ggvis(~carat, ~price, fill = ~cut) %>% layer_points()
# Petal length vs width
dia %>% ggvis(~carat, ~price, fill = ~cut) %>% layer_points()
# Sepal length vs width
dia %>% ggvis(~carat, ~color, fill = ~cut) %>% layer_points()
# Petal length vs width
dia %>% ggvis(~carat, ~depth, fill = ~cut) %>% layer_points()
# Sepal length vs width
dia %>% ggvis(~price, ~color, fill = ~cut) %>% layer_points()
# Sepal length vs width
dia %>% ggvis(~clarity, ~color, fill = ~cut) %>% layer_points()
# Sepal length vs width
dia %>% ggvis(~clarity, ~price, fill = ~cut) %>% layer_points()
# Sepal length vs width
dia %>% ggvis(~price, ~table, fill = ~cut) %>% layer_points()
##create a random number equal 90% of total number of rows
ran <- sample(1:nrow(dia),0.9 * nrow(dia))
##the normalization function is created
nor <-function(x) { (x -min(x))/(max(x)-min(x))   }
##normalization function is created
dia_nor <- as.data.frame(lapply(dia[,c(1,5,6,7,8,9,10)], nor))
##training dataset extracted
dia_train <- dia_nor[ran,]
##test dataset extracted
dia_test <- dia_nor[-ran,]
##the 2nd column of training dataset because that is what we need to predict about testing dataset
##also convert ordered factor to normal factor
dia_target <- as.factor(dia[ran,2])
##the actual values of 2nd couln of testing dataset to compaire it with values that will be predicted
##also convert ordered factor to normal factor
test_target <- as.factor(dia[-ran,2])
pr <- knn(dia_train,dia_test,cl=dia_target,k=20)
##create the confucion matrix
tb <- table(pr,test_target)
##check the accuracy
accuracy <- function(x){sum(diag(x)/(sum(rowSums(x)))) * 100}
accuracy(tb)
tb
boston = read.csv('boston.csv')
boston = read.csv('boston.csv')
str(boston)
# Plot observations
plot(boston$LON, boston$LAT)
# Tracts alongside the Charles River
points(boston$LON[boston$CHAS==1], boston$LAT[boston$CHAS==1], col="blue", pch=19)
# Plot pollution/NOX
summary(boston$NOX)
points(boston$LON[boston$NOX>=0.55], boston$LAT[boston$NOX>=0.55], col="green", pch=20)
# Plot prices
plot(boston$LON, boston$LAT)
summary(boston$MEDV)
#Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
#5.00   17.02   21.20   22.53   25.00   50.00
points(boston$LON[boston$MEDV>=21.2], boston$LAT[boston$MEDV>=21.2], col="red", pch=20)
# Load CART packages
library(rpart)
# install rpart package
#install.packages("rpart.plot")
library(rpart.plot)
# CART model
latlontree = rpart(MEDV ~ LAT + LON, data=boston)
# Plot the tree using prp command defined in rpart.plot package
prp(latlontree)
# Visualize output
plot(boston$LON, boston$LAT)
points(boston$LON[boston$MEDV>=21.2],boston$LAT[boston$MEDV>=21.2], col="red", pch=20)
fittedvalues = predict(latlontree)
points(boston$LON[fittedvalues>21.2],boston$LAT[fittedvalues>=21.2], col="blue", pch="$")
#Simplifying Tree by increasing minbucket
latlontree = rpart(MEDV ~ LAT + LON, data=boston, minbucket=50)
plot(latlontree)
text(latlontree)
fittedvalues = predict(latlontree)
points(boston$LON[fittedvalues>21.2],boston$LAT[fittedvalues>=21.2], col="blue", pch="$")
# Visualize output
plot(boston$LON, boston$LAT)
points(boston$LON[boston$MEDV>=21.2],boston$LAT[boston$MEDV>=21.2], col="red", pch=20)
fittedvalues = predict(latlontree)
points(boston$LON[fittedvalues>21.2],boston$LAT[fittedvalues>=21.2], col="blue", pch="$")
# Split the data
#install.packages("caTools")
library(caTools)
set.seed(123)
split = sample.split(boston$MEDV, SplitRatio = 0.7)
train = subset(boston, split==TRUE)
test = subset(boston, split==FALSE)
# Making a Regression Tree Model
# Create a CART model
tree = rpart(MEDV ~ LAT + LON + CRIM + ZN + INDUS + CHAS + NOX + RM + AGE + DIS + RAD + TAX + PTRATIO, data=train)
prp(tree)
tree.pred = predict(tree, newdata=test)
tree.sse = sum((tree.pred - test$MEDV)^2)
tree.sse
# Now use the IRIS dataset to classify iris type using CART
# load the package
library(rpart)
# load data
data(iris)
# fit model
fit <- rpart(Species~., data=iris)
# summarize the fit
summary(fit)
# make predictions
predictions <- predict(fit, iris[,1:4], type="class")
# summarize accuracy
iris_pred <- table(predictions, iris$Species)
iris_pred
##check the accuracy
accuracy <- function(x){sum(diag(x)/(sum(rowSums(x)))) * 100}
accuracy(iris_pred)
# Install packages/library and read in data
#install.packages("kableExtra")
library(kableExtra)
library(knitr)
credit.df <- read.csv('credit.csv')
credit.df$default <- factor(credit.df$default, levels = c(1, 2), labels = c('Yes', 'No'))
#View and understand you data
str(credit.df)
summary(credit.df)
kable(head(credit.df, n=15), format="html") %>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))
set.seed(1978)
credit.random.df <- credit.df[order(runif(1000)), ]
summary(credit.df$amount)
summary(credit.random.df$amount)
credit.train <- credit.random.df[1:900, ]
credit.test <- credit.random.df[901:1000, ]
prop.table(table(credit.train$default))
prop.table(table(credit.test$default))
library(C50)
credit.fit <- C5.0(credit.train[-17], credit.train$default)
credit.fit
summary(credit.fit)
dim(boston)
11+18+18+22+30+28+34+45
kable(head(credit.df, n=15), format="html") %>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))
summary(credit.fit)
